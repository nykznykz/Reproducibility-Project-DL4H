{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b71d209-f3a8-48d7-a9a2-18f579e9efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52641815-71a7-429c-b816-1be781df468a",
   "metadata": {},
   "source": [
    "### Load preprocessed data to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c791c5-cd5c-4d90-ac4c-e560aa3e468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"los.pkl\",\"rb\") as f:\n",
    "    los_7 = pickle.load(f)\n",
    "\n",
    "with open(\"mort.pkl\",\"rb\") as f:\n",
    "    mort = pickle.load(f)\n",
    "    \n",
    "with open(\"intervention_seq.pkl\",\"rb\") as f:\n",
    "    seqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaea9f4-0742-47c1-9359-02e95b124a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, labels):\n",
    "        self.x = seqs\n",
    "        self.y = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return self.x[index], self.y[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657123e1-2945-457c-933c-3d41bf63b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f54f083-a63f-4247-8a10-864e36425c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34472"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ae59f1-3311-4632-84cb-8671e480a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_interventions = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(intervention) for patient in sequences for intervention in patient]\n",
    "\n",
    "    max_num_interventions = max(num_interventions)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_intervention, intervention in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            # your code here\n",
    "            for idx, code in enumerate(intervention):\n",
    "                x[i_patient,j_intervention,idx] = code\n",
    "                masks[i_patient,j_intervention,idx] = 1\n",
    "                rev_x[i_patient, len(patient)-j_intervention-1, idx] = code\n",
    "                rev_masks[i_patient, len(patient)-j_intervention-1, idx] = 1\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5a61cb-e0f8-4e6b-a6ac-e2d613d81af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18643d2a-fe19-4c6b-b318-b8248ce9bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e169fadd-71c2-4156-a1ff-c04a63a65fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d79daa8-d213-45f0-8ad5-36d23a7eba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    x_masked = x * masks.unsqueeze(dim=-1)\n",
    "    # your code here\n",
    "    return x_masked.sum(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55113e99-3e57-4798-a48b-f204ee4cd411",
   "metadata": {},
   "source": [
    "### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a970d9a3-9dc3-40ea-9dd1-ea2aaa17ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: Consider using `torch.gather()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=2),axis=1)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae3bccd-2f39-4a0b-8b4b-540fd30a87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.embedding = nn.Embedding(num_codes+1,embedding_dim=128)\n",
    "        self.rnn = nn.LSTM(128, 128, 1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        \n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, (hn, cn) = self.rnn(x)\n",
    "        # hn = hn.view(batch_size,-1)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order, and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "#         # 5.1 Pass the sequence through the embedding layer;\n",
    "#         rev_x = self.embedding(rev_x)\n",
    "#         # 5.2 Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "#         rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        \n",
    "#         # 5.3 Pass the embegginds through the RNN layer;\n",
    "#         output, _ = self.rnn(rev_x)\n",
    "#         # 5.4 Obtain the hidden state at the last visit.\n",
    "#         true_h_n_rev = get_last_visit(output, rev_masks)\n",
    "        \n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        x = self.relu(self.fc1(true_h_n))\n",
    "        out = self.fc2(x)\n",
    "        return out.view(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ea00cb-e8a0-4ee3-9f06-94bc62ede7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = NaiveRNN(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7efd8b9-eb2d-4fd4-9594-5f3c006c8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([50]))\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd9fae32-0c0a-4537-b34a-10b4e5e1ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, _, _, y in val_loader:\n",
    "        y_hat = model(x, masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    p, r, f,_ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score, average='macro')\n",
    "\n",
    "    return p, r, f, roc_auc, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "661c73d3-e47d-467e-99dd-a6fb1c026d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x, masks)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, auprc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, auprc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc,auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d2a43-8e9b-4a91-b931-404d63059bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66697b6-663d-4ffd-8054-7619612529e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, auprc = eval_model(rnn, val_loader)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd5e24-0929-4e4e-a9a8-250b15b45a66",
   "metadata": {},
   "source": [
    "### Demographics injected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59b6071-4d88-4b3e-9da4-732295c21110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"los.pkl\",\"rb\") as f:\n",
    "    los_7 = pickle.load(f)\n",
    "\n",
    "with open(\"demograph_intervention_seq.pkl\",\"rb\") as f:\n",
    "    demo_seqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ac95f7-5d15-4e6e-81f2-f8cdc85e59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_dataset = CustomDataset(demo_seqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "259186ee-e0d4-4bb9-8fa3-757be241eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(demo_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(demo_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(demo_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee57be8-6c28-4643-baad-5623bd59ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb7fb6ca-4cc7-4931-a888-abbbb16b7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = NaiveRNN(14+49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e182c685-d539-4636-8388-7690337cf558",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([40]))\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb4e3b-433b-46b5-af92-993243fadd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efb0d8b1-7db2-4c68-b1c3-cf240cf664f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7622375128490214\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(rnn, val_loader)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c3b3845-3a0c-489b-aee4-a49a4cb41a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17354109567339437\n"
     ]
    }
   ],
   "source": [
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd0709-a024-4425-b998-721874493e58",
   "metadata": {},
   "source": [
    "### 2 modal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcff517d-9083-4c2a-bbe6-54e885cd8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"los.pkl\",\"rb\") as f:\n",
    "    los_7 = pickle.load(f)\n",
    "\n",
    "with open(\"mort.pkl\",\"rb\") as f:\n",
    "    mort = pickle.load(f)\n",
    "\n",
    "with open(\"demograph_intervention_seq.pkl\",\"rb\") as f:\n",
    "    demo_iseqs = pickle.load(f)\n",
    "    \n",
    "with open(\"demograph_vitals_zero_seq.pkl\",\"rb\") as f:\n",
    "    demo_vseqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084afe16-e77b-49e0-b9ac-6a37e8909068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs1, seqs2, labels):\n",
    "        self.x1 = seqs1\n",
    "        self.x2 = seqs2\n",
    "        self.y = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return self.x1[index], self.x2[index], self.y[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c5d3cc8-39e3-4f29-b9ce-a8dc7e46ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sequences1, sequences2, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences1)\n",
    "    num_interventions = [len(patient) for patient in sequences1]\n",
    "    num_codes = [len(intervention) for patient in sequences1 for intervention in patient]\n",
    "\n",
    "    max_num_interventions = max(num_interventions)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x1 = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.long)\n",
    "    masks1 = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "    for i_patient, patient in enumerate(sequences1):\n",
    "        for j_intervention, intervention in enumerate(patient):\n",
    "            # your code here\n",
    "            for idx, code in enumerate(intervention):\n",
    "                x1[i_patient,j_intervention,idx] = code\n",
    "                masks1[i_patient,j_intervention,idx] = 1\n",
    "    \n",
    "\n",
    "    num_vitals = [len(patient) for patient in sequences2]\n",
    "    max_num_vitals = max(num_interventions)\n",
    "    vitals_dim = 107\n",
    "    x2 = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.long)\n",
    "    masks2 = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences2):\n",
    "        for j_vitals, vitals in enumerate(patient):\n",
    "            # your code here\n",
    "            x2[i_patient, j_vitals] = torch.tensor(vitals, dtype=torch.long)\n",
    "            masks2[i_patient, j_vitals] = 1\n",
    "    \n",
    "    return x1, masks1, x2, masks2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea5ef-a208-4739-b1a6-b35ce32e8f48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "982d2474-fc5e-4810-a0db-73ac6f49a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LF_RNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        self.m1_rnn = nn.LSTM(128, 128, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        self.m2_rnn = nn.LSTM(128, 128, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1, masks1, x2, masks2):\n",
    "        batch_size = x1.shape[0]\n",
    "        \n",
    "        # FIRST MODAL\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1 = self.m1_embedding(x1)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1 = sum_embeddings_with_mask(x1, masks1)\n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output1, (hn1, cn1) = self.m1_rnn(x1)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n1 = get_last_visit(output1, masks1)\n",
    "        \n",
    "        # SECOND MODAL\n",
    "        num_x2 = x2[:,:,:-3]\n",
    "        cat_x2 = x2[:,:,-3:]\n",
    "        num_x2 = self.m2_embedding_num(num_x2.float())\n",
    "        cat_x2 = self.m2_embedding_cat(cat_x2)\n",
    "        cat_x2 = cat_x2.sum(dim=2)\n",
    "        x2 = torch.concat([num_x2, cat_x2], dim=-1)\n",
    "        output2, (hn2, cn2) = self.m2_rnn(x2)\n",
    "        true_h_n2 = get_last_visit(output2, masks2)\n",
    "        \n",
    "        # LATE FUSION\n",
    "        # concat hidden stats and pass through \n",
    "        true_h_n = torch.concat([true_h_n1, true_h_n2], dim=-1)\n",
    "        x = self.relu(self.fc1(true_h_n))\n",
    "        out = self.fc2(x)\n",
    "        return out.view(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daef0aa8-b4a5-4ad9-8932-97c97afce050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x1, masks1, x2, masks2, y in val_loader:\n",
    "        y_hat = model(x1, masks1, x2, masks2)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    p, r, f,_ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score, average='macro')\n",
    "\n",
    "    return p, r, f, roc_auc, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc3ccb6c-484a-4d16-9af7-7c4c969344a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x1, masks1, x2, masks2, y in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x1, masks1, x2, masks2)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, auprc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, auprc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc,auprc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae73f9-07b1-4bc4-bb6d-9d5286c6a9c2",
   "metadata": {},
   "source": [
    "LOS_7 Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d22bbd-af6f-461a-9b57-4457312328e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d7086c8-0343-4866-8e58-a86e9c82cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3340b395-bbcd-4be2-be49-dbecb5bf509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82af0202-c1d4-4436-a720-5db764b0931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfrnn = LF_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ec5b93f-b2fe-4cb6-a513-b99981196782",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(lfrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57c890ed-0615-4286-a668-3e4ce2869694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.378654\n",
      "Epoch: 1 \t Validation p: 0.13, r:0.77, f: 0.22, roc_auc: 0.80, auprc: 0.19\n",
      "Epoch: 2 \t Training Loss: 1.294997\n",
      "Epoch: 2 \t Validation p: 0.18, r:0.61, f: 0.28, roc_auc: 0.80, auprc: 0.19\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(lfrnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9ef6e89-bee3-4e6d-888a-8b535027ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028271566130574\n",
      "0.19366082258062162\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(lfrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935ee8a-0d25-43f1-b789-fd345cb7176c",
   "metadata": {},
   "source": [
    "ICU MORT Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdd3531a-fc98-4b81-9b52-3a2d143c9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0539df9a-e921-4492-b04e-6c58f878d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f888bd0f-97f0-485a-97fc-f49b0bbbbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "459429b9-d7eb-47df-8161-88d2e245e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfrnn = LF_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1baec51e-beff-4fc7-865e-bacb5c979a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(lfrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f9fa3d7-1fd6-4e84-821c-7b46009b6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.268148\n",
      "Epoch: 1 \t Validation p: 0.20, r:0.85, f: 0.33, roc_auc: 0.89, auprc: 0.47\n",
      "Epoch: 2 \t Training Loss: 1.000276\n",
      "Epoch: 2 \t Validation p: 0.24, r:0.82, f: 0.37, roc_auc: 0.90, auprc: 0.50\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(lfrnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8c95006-8ac0-4619-8b88-84f715f5546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8975373213154003\n",
      "0.5009417032007952\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(lfrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf88688-a262-401d-a61e-0018fafb58b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e950d0-078e-4954-9273-bc11aaf212b7",
   "metadata": {},
   "source": [
    "#### Early Fusion on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fef3eefd-1a2b-4c23-a0d3-a2828b1ce79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=-1),axis=0)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a935ad34-de40-4765-8ffd-3d33577a2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EF_RNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        \n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        \n",
    "        self.rnn = nn.LSTM(256, 128, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1, masks1, x2, masks2):\n",
    "        batch_size = x1.shape[0]\n",
    "\n",
    "        # FIRST MODAL\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1 = self.m1_embedding(x1)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1 = sum_embeddings_with_mask(x1, masks1)\n",
    "        \n",
    "        # SECOND MODAL\n",
    "        num_x2 = x2[:,:,:-3]\n",
    "        cat_x2 = x2[:,:,-3:]\n",
    "        num_x2 = self.m2_embedding_num(num_x2.float())\n",
    "        cat_x2 = self.m2_embedding_cat(cat_x2)\n",
    "        cat_x2 = cat_x2.sum(dim=2)\n",
    "        x2 = torch.concat([num_x2, cat_x2], dim=-1)\n",
    "        \n",
    "        # 3. FUSE MODALS and Pass the embegginds through the RNN layer\n",
    "        x3 = torch.concat([x1, x2], dim=-1)\n",
    "        output, (hn, cn) = self.rnn(x3)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        fused_mask = torch.logical_or(masks1.sum(dim=-1),masks2.sum(dim=-1))\n",
    "        true_h_n = get_last_visit(output, fused_mask)\n",
    "        \n",
    "        x = self.relu(self.fc1(true_h_n))\n",
    "        out = self.fc2(x)\n",
    "        return out.view(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb7de9-9d10-4fd7-a819-f5ebc5ca419b",
   "metadata": {},
   "source": [
    "LOS 7 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e757f83-cc93-4dae-9a04-ef4dda78d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87054b20-dfff-436f-ac7e-586ede54cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "967ffba8-6935-4edc-9e1b-ba6ead811bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e1e7eed-a09a-4ff0-b254-5caa95ef00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "efrnn = EF_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56b80596-1fca-4525-a705-c7ed2af42117",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(efrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89b1bc0a-e34c-4ff2-b428-d910358c23bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.529512\n",
      "Epoch: 1 \t Validation p: 0.13, r:0.67, f: 0.22, roc_auc: 0.77, auprc: 0.17\n",
      "Epoch: 2 \t Training Loss: 1.411297\n",
      "Epoch: 2 \t Validation p: 0.11, r:0.82, f: 0.19, roc_auc: 0.78, auprc: 0.16\n",
      "Epoch: 3 \t Training Loss: 1.356769\n",
      "Epoch: 3 \t Validation p: 0.11, r:0.84, f: 0.20, roc_auc: 0.79, auprc: 0.17\n",
      "Epoch: 4 \t Training Loss: 1.328977\n",
      "Epoch: 4 \t Validation p: 0.16, r:0.67, f: 0.25, roc_auc: 0.80, auprc: 0.19\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 4\n",
    "train(efrnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6802f25-6ce8-4852-994c-cb46a7f09cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7969238598817623\n",
      "0.1857725093636958\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(efrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1982de-c7d6-481e-b1eb-4ce8562a39d9",
   "metadata": {},
   "source": [
    "ICU MORT task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0af06a12-4036-40db-94a1-a1a53bcc09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b2993f2-7022-4c1b-aac3-00489be1c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b0f3c74-a1fa-4927-8a73-a6ebd5386e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3b1d77b-102e-4e9c-ad20-022d5c6e88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "efrnn = EF_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62ae646f-0aef-406b-b350-8ead46716894",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(efrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e09767d1-06f5-423f-b3a8-a4ecaace3993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.440768\n",
      "Epoch: 1 \t Validation p: 0.18, r:0.88, f: 0.29, roc_auc: 0.87, auprc: 0.36\n",
      "Epoch: 2 \t Training Loss: 1.180359\n",
      "Epoch: 2 \t Validation p: 0.17, r:0.86, f: 0.29, roc_auc: 0.87, auprc: 0.40\n",
      "Epoch: 3 \t Training Loss: 1.134974\n",
      "Epoch: 3 \t Validation p: 0.26, r:0.70, f: 0.38, roc_auc: 0.87, auprc: 0.39\n",
      "Epoch: 4 \t Training Loss: 1.089461\n",
      "Epoch: 4 \t Validation p: 0.25, r:0.74, f: 0.38, roc_auc: 0.89, auprc: 0.43\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 4\n",
    "train(efrnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35c8c32e-b007-4f27-b539-aadd612778b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8850463748549182\n",
      "0.4290843993476328\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(efrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaea85-6b3e-44ab-b5cd-0fad571ae737",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e680ea8b-f824-4452-985a-f707ef045447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LF_TF(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        encoder_layer1 = nn.TransformerEncoderLayer(d_model=128, nhead=2)\n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=2)\n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(d_model=128, nhead=2)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1, masks1, x2, masks2):\n",
    "        batch_size = x1.shape[0]\n",
    "        \n",
    "        # FIRST MODAL\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1 = self.m1_embedding(x1)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1 = sum_embeddings_with_mask(x1, masks1)\n",
    "        # 3. Pass the embegginds through the TF layer;\n",
    "        output1 = self.transformer_encoder1(x1)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n1 = get_last_visit(output1, masks1)\n",
    "        \n",
    "        # SECOND MODAL\n",
    "        num_x2 = x2[:,:,:-3]\n",
    "        cat_x2 = x2[:,:,-3:]\n",
    "        num_x2 = self.m2_embedding_num(num_x2.float())\n",
    "        cat_x2 = self.m2_embedding_cat(cat_x2)\n",
    "        cat_x2 = cat_x2.sum(dim=2)\n",
    "        x2 = torch.concat([num_x2, cat_x2], dim=-1)\n",
    "        output2 = self.transformer_encoder2(x2)\n",
    "        true_h_n2 = get_last_visit(output2, masks2)\n",
    "        \n",
    "        # LATE FUSION\n",
    "        # concat hidden stats and pass through \n",
    "        true_h_n = torch.concat([true_h_n1, true_h_n2], dim=-1)\n",
    "        x = self.relu(self.fc1(true_h_n))\n",
    "        out = self.fc2(x)\n",
    "        return out.view(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eac996f-850b-4308-8803-c2cb196e157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534194fe-fccf-4c26-ae6b-2672ebd95286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5af022a-3d43-4980-9b1c-fea4270d71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d45b7a-813e-4876-808e-51ec09c36779",
   "metadata": {},
   "outputs": [],
   "source": [
    "lftf = LF_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4db071d2-92e6-4fb1-ad13-19631e2a4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(lftf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747115c-026d-43fd-b8b2-3b014d99120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(lftf, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5050caad-0abe-49f7-a082-93c3ab6e9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480324915141734\n",
      "0.17193860381483445\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(lftf, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c14115dd-1ab2-453a-b04c-eb3ab1805412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EF_TF(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        \n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1, masks1, x2, masks2):\n",
    "        batch_size = x1.shape[0]\n",
    "\n",
    "        # FIRST MODAL\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1 = self.m1_embedding(x1)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1 = sum_embeddings_with_mask(x1, masks1)\n",
    "        \n",
    "        # SECOND MODAL\n",
    "        num_x2 = x2[:,:,:-3]\n",
    "        cat_x2 = x2[:,:,-3:]\n",
    "        num_x2 = self.m2_embedding_num(num_x2.float())\n",
    "        cat_x2 = self.m2_embedding_cat(cat_x2)\n",
    "        cat_x2 = cat_x2.sum(dim=2)\n",
    "        x2 = torch.concat([num_x2, cat_x2], dim=-1)\n",
    "        \n",
    "        # 3. FUSE MODALS and Pass the embegginds through the RNN layer\n",
    "        x3 = torch.concat([x1, x2], dim=-1)\n",
    "        output = self.transformer_encoder(x3)\n",
    "\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        fused_mask = torch.logical_or(masks1.sum(dim=-1),masks2.sum(dim=-1))\n",
    "        true_h_n = get_last_visit(output, fused_mask)\n",
    "        \n",
    "        x = self.relu(self.fc1(true_h_n))\n",
    "        out = self.fc2(x)\n",
    "        return out.view(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34bff46d-ef9a-4489-8d80-8309c972222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=-1),axis=0)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "029a0063-87bd-4e30-b87c-aa1b0b4664cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eftf = EF_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "665d98d8-2978-43ce-9aea-5a90a7ca3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(eftf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ea3591d-ce86-4efd-baff-e24222d61e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.662546\n",
      "Epoch: 1 \t Validation p: 0.10, r:0.40, f: 0.16, roc_auc: 0.60, auprc: 0.09\n",
      "Epoch: 2 \t Training Loss: 1.648254\n",
      "Epoch: 2 \t Validation p: 0.12, r:0.29, f: 0.17, roc_auc: 0.62, auprc: 0.10\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(eftf, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b930985-2c5f-4a53-bfed-27a97578c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222487912219276\n",
      "0.1029306517332224\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(eftf, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04f0e1-5b05-4e8b-a34d-2ab8eb9bae67",
   "metadata": {},
   "source": [
    "MORT ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fee79f3-5059-4943-aaf8-cc0a0519a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: Consider using `torch.gather()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=2),axis=1)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1ad592f-2a87-4653-bc93-ddb8c42c13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51bb21a7-a3a3-47c2-afeb-5e1f2105b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b61f817a-2ef7-4186-88cb-c8fada90b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aad4a039-14ea-4d26-821e-aac4adc2b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "lftf = LF_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7c73210-554a-475e-b536-a77df3e9f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(lftf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a280c7c8-ac6f-4c8d-8bde-a6e48d09441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.557366\n",
      "Epoch: 1 \t Validation p: 0.15, r:0.77, f: 0.25, roc_auc: 0.79, auprc: 0.26\n",
      "Epoch: 2 \t Training Loss: 1.465113\n",
      "Epoch: 2 \t Validation p: 0.16, r:0.71, f: 0.26, roc_auc: 0.80, auprc: 0.28\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(lftf, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5b1999a-8175-4245-b5e7-867f4f63f4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797688079444194\n",
      "0.2753349927884807\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(lftf, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6908b98b-f8a7-4209-9330-dfce315a49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=-1),axis=0)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2361fbd-514e-4046-9fce-117bbb408e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "eftf = EF_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "188ba83d-04ca-45a2-9146-6a5024f3eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([30]))\n",
    "optimizer = torch.optim.Adam(eftf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2d164b5-e5b0-412a-a10f-a8367ba2f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.749289\n",
      "Epoch: 1 \t Validation p: 0.12, r:0.66, f: 0.21, roc_auc: 0.73, auprc: 0.17\n",
      "Epoch: 2 \t Training Loss: 1.638730\n",
      "Epoch: 2 \t Validation p: 0.16, r:0.59, f: 0.25, roc_auc: 0.75, auprc: 0.20\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2\n",
    "train(eftf, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c393989c-cc78-4370-bc0f-ff7769625ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7518612962759859\n",
      "0.19828850440438192\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(eftf, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9da2a-bd9f-412e-aa5d-743735688df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163333b-1a02-4dce-bb4d-e608cde06d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da3ae8-e0e4-4855-85ec-49389af22928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967c2ed-3f90-4d8d-bf34-10ebc7542345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
