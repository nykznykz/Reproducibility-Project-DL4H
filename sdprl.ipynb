{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca92ad79-2805-4b1c-bf9a-9b5d72a65999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f7023-7b01-4fb0-b2d9-c0a32a62c804",
   "metadata": {},
   "source": [
    "### Load preprocessed data to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b52525d-1e63-45f3-a434-654851e3b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"los.pkl\",\"rb\") as f:\n",
    "    los_7 = pickle.load(f)\n",
    "    \n",
    "with open(\"mort.pkl\",\"rb\") as f:\n",
    "    mort = pickle.load(f)\n",
    "    \n",
    "with open(\"demograph_interventions_SDPRL.pkl\",\"rb\") as f:\n",
    "    demo_iseqs = pickle.load(f)\n",
    "\n",
    "with open(\"demograph_vitals_zero_SDPRL.pkl\",\"rb\") as f:\n",
    "    demo_vseqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b82056f-c0a3-43cf-89c0-51cce3733185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs1, seqs2, labels):\n",
    "        self.x1 = seqs1\n",
    "        self.x2 = seqs2\n",
    "        self.y = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # SDPRL: m1_pos,m2_pos, m1_neg,m2_neg, y\n",
    "        return self.x1[index][0], self.x2[index][0], self.x1[index][1], self.x2[index][1], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3239bc33-2449-4733-8f1c-145907f31997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235afe71-e92d-4948-bcd8-2c68add3aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sequences1pos, sequences2pos,sequences1neg, sequences2neg, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    ########################################################\n",
    "    #padding and masking for modal 1 & 2 for positive sample\n",
    "    ########################################################\n",
    "    num_patients = len(sequences1pos)\n",
    "    num_interventions = [len(patient) for patient in sequences1pos]\n",
    "    num_codes = [len(intervention) for patient in sequences1pos for intervention in patient]\n",
    "\n",
    "    max_num_interventions = max(num_interventions)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x1_pos = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.long)\n",
    "    masks1_pos = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "    for i_patient, patient in enumerate(sequences1pos):\n",
    "        for j_intervention, intervention in enumerate(patient):\n",
    "            # your code here\n",
    "            for idx, code in enumerate(intervention):\n",
    "                x1_pos[i_patient,j_intervention,idx] = code\n",
    "                masks1_pos[i_patient,j_intervention,idx] = 1\n",
    "    \n",
    "\n",
    "    num_vitals = [len(patient) for patient in sequences2pos]\n",
    "    max_num_vitals = max(num_interventions)\n",
    "    vitals_dim = 107\n",
    "    x2_pos = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.long)\n",
    "    masks2_pos = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences2pos):\n",
    "        for j_vitals, vitals in enumerate(patient):\n",
    "            # your code here\n",
    "            x2_pos[i_patient, j_vitals] = torch.tensor(vitals, dtype=torch.long)\n",
    "            masks2_pos[i_patient, j_vitals] = 1\n",
    "            \n",
    "    ########################################################\n",
    "    #padding and masking for modal 1 & 2 for NEGATIVE sample\n",
    "    ########################################################\n",
    "    num_patients = len(sequences1neg)\n",
    "    num_interventions = [len(patient) for patient in sequences1neg]\n",
    "    num_codes = [len(intervention) for patient in sequences1neg for intervention in patient]\n",
    "\n",
    "    max_num_interventions = max(num_interventions)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x1_neg = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.long)\n",
    "    masks1_neg = torch.zeros((num_patients, max_num_interventions, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "    for i_patient, patient in enumerate(sequences1neg):\n",
    "        for j_intervention, intervention in enumerate(patient):\n",
    "            # your code here\n",
    "            for idx, code in enumerate(intervention):\n",
    "                x1_neg[i_patient,j_intervention,idx] = code\n",
    "                masks1_neg[i_patient,j_intervention,idx] = 1\n",
    "    \n",
    "\n",
    "    num_vitals = [len(patient) for patient in sequences2neg]\n",
    "    max_num_vitals = max(num_interventions)\n",
    "    vitals_dim = 107\n",
    "    x2_neg = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.long)\n",
    "    masks2_neg = torch.zeros((num_patients, max_num_vitals, vitals_dim), dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences2neg):\n",
    "        for j_vitals, vitals in enumerate(patient):\n",
    "            # your code here\n",
    "            x2_neg[i_patient, j_vitals] = torch.tensor(vitals, dtype=torch.long)\n",
    "            masks2_neg[i_patient, j_vitals] = 1\n",
    "    \n",
    "    return x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "            x1_neg, masks1_neg, x2_neg, masks2_neg, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccb6b2e-0e2c-476d-89b8-a3e74116d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103e12cd-0555-4223-9c70-fd2f5d8c61cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502a0ba6-1101-4e68-a0d8-3b3c2dd6b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2971c49a-8652-4f29-b476-441fdc5ac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "            x1_neg, masks1_neg, x2_neg, masks2_neg, y = next(loader_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74936fbe-fdbf-4e0d-9fcc-ddcb4c138ca6",
   "metadata": {},
   "source": [
    "### SDPRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194c9dbd-35c4-4856-88a8-fb52aa1daac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    x_masked = x * masks.unsqueeze(dim=-1)\n",
    "    # your code here\n",
    "    return x_masked.sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea5f705-0f3b-41bd-a286-14a223eee8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: Consider using `torch.gather()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    first_zero_idx = torch.argmin(masks.sum(dim=2),axis=1)\n",
    "    last_nonzero_idx = first_zero_idx - 1\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    return hidden_states[list(range(batch_size)), last_nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270e60e1-4c88-4dbe-87ce-237fcfc74681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPRL_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        self.m1_rnn = nn.LSTM(128, 128, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        self.m2_rnn = nn.LSTM(128, 128, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1_pos, masks1_pos, x2_pos, masks2_pos, x1_neg=None, masks1_neg=None, x2_neg=None, masks2_neg=None):\n",
    "        batch_size = x1_pos.shape[0]\n",
    "        \n",
    "        # FIRST MODAL pos\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1_pos = self.m1_embedding(x1_pos)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1_pos = sum_embeddings_with_mask(x1_pos, masks1_pos)\n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output1_pos, (hn1, cn1) = self.m1_rnn(x1_pos)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n1_pos = get_last_visit(output1_pos, masks1_pos)\n",
    "        \n",
    "        # SECOND MODAL pos\n",
    "        num_x2_pos = x2_pos[:,:,:-3]\n",
    "        cat_x2_pos = x2_pos[:,:,-3:]\n",
    "        num_x2_pos = self.m2_embedding_num(num_x2_pos.float())\n",
    "        cat_x2_pos = self.m2_embedding_cat(cat_x2_pos)\n",
    "        cat_x2_pos = cat_x2_pos.sum(dim=2)\n",
    "        x2_pos = torch.concat([num_x2_pos, cat_x2_pos], dim=-1)\n",
    "        output2_pos, (hn2, cn2) = self.m2_rnn(x2_pos)\n",
    "        true_h_n2_pos = get_last_visit(output2_pos, masks2_pos)\n",
    "        \n",
    "        # FOR PREDICTION\n",
    "        # concat hidden stats and pass through \n",
    "        true_h_n_pos = torch.maximum(true_h_n1_pos, true_h_n2_pos)\n",
    "        x = self.relu(self.fc1(true_h_n_pos))\n",
    "        out_pos = self.fc2(x).view(batch_size)\n",
    "        \n",
    "        if x1_neg != None:\n",
    "            # FIRST MODAL neg\n",
    "            # 1. Pass the sequence through the embedding layer;\n",
    "            x1_neg = self.m1_embedding(x1_neg)\n",
    "            # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "            x1_neg = sum_embeddings_with_mask(x1_neg, masks1_neg)\n",
    "            # 3. Pass the embegginds through the RNN layer;\n",
    "            output1_neg, (hn1, cn1) = self.m1_rnn(x1_neg)\n",
    "            # 4. Obtain the hidden state at the last visit.\n",
    "            true_h_n1_neg = get_last_visit(output1_neg, masks1_neg)\n",
    "\n",
    "            # SECOND MODAL pos\n",
    "            num_x2_neg = x2_neg[:,:,:-3]\n",
    "            cat_x2_neg = x2_neg[:,:,-3:]\n",
    "            num_x2_neg = self.m2_embedding_num(num_x2_neg.float())\n",
    "            cat_x2_neg = self.m2_embedding_cat(cat_x2_neg)\n",
    "            cat_x2_neg = cat_x2_neg.sum(dim=2)\n",
    "            x2_neg = torch.concat([num_x2_neg, cat_x2_neg], dim=-1)\n",
    "            output2_neg, (hn2, cn2) = self.m2_rnn(x2_neg)\n",
    "            true_h_n2_neg = get_last_visit(output2_neg, masks2_neg)\n",
    "\n",
    "            # FOR TRAINING/Calculating loss\n",
    "            # lm1m2\n",
    "            exp_pos = torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_pos, true_h_n2_pos) / TAU)\n",
    "            lm1m2_denom = 32 * torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_pos, true_h_n2_neg) / TAU)\n",
    "            lm1m2 = -torch.log(exp_pos/lm1m2_denom)\n",
    "            # lm2m1\n",
    "            lm2m1_denom = 32 * torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_neg, true_h_n2_pos) / TAU)\n",
    "            lm2m1 = -torch.log(exp_pos/lm2m1_denom)    \n",
    "            return out_pos, lm1m2, lm2m1\n",
    "            \n",
    "            \n",
    "        \n",
    "        # prediction mode\n",
    "        return out_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a10bdbc-7991-44a8-a787-ab26e02def5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "        x1_neg, masks1_neg, x2_neg, masks2_neg, y in val_loader:\n",
    "        \n",
    "        y_hat = model(x1_pos, masks1_pos, x2_pos, masks2_pos)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    # p, r, f,_ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    p, r, f = None, None, None\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score, average='macro')\n",
    "\n",
    "    return p, r, f, roc_auc, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b99617ba-d510-4e9a-9579-e2d9fc258708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "            x1_neg, masks1_neg, x2_neg, masks2_neg, y in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, lm1m2, lm2m1 = model(x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "                                        x1_neg, masks1_neg, x2_neg, masks2_neg)\n",
    "            task_loss = task_criterion(y_pred, y)\n",
    "            loss = lambda1 * task_loss + lambda2 * torch.mean(lm1m2) + lambda3 * torch.mean(lm2m1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, auprc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation roc_auc: {:.4f}, auprc: {:.4f}'\n",
    "              .format(epoch+1,roc_auc,auprc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b83384-0442-4408-8d59-794446767932",
   "metadata": {},
   "source": [
    "LOS 7 TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6626b993-8574-470f-95cd-1b752308e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.1\n",
    "lambda1 = 0.05\n",
    "lambda2 = 0.05\n",
    "lambda3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9645b5-8939-4041-b43a-a43fb5ef3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdprlrnn = SDPRL_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0c86ac-0d7f-4016-856c-8e137124afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(sdprlrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a964ce-4390-4765-b76c-a7d5257f153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(sdprlrnn, train_loader, val_loader, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b284e46-a58b-49b5-a70c-9a19bf304aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7897234784855925\n",
      "0.1920386786378866\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(sdprlrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a113d88-93e7-4c39-bf90-b45ff22c9af6",
   "metadata": {},
   "source": [
    "ICU MORT TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12376378-d178-402a-8fce-864404f88fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1dde00f-434d-4079-a866-673621c8d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ef51c91-2bfc-4713-a348-310ea884d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c407a1d2-2662-4b03-8ecd-4acf1569062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdprlrnn = SDPRL_RNN(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72a01ec3-b82d-4080-83e7-8bc84c3cf91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(sdprlrnn.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906d0c7-8ac1-465e-99db-0cbed061a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(sdprlrnn, train_loader, val_loader, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc07263b-caf1-4618-b99a-096664f8d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8013975051378864\n",
      "0.2270470319798003\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(sdprlrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2709e-1bcf-4ea4-b47a-dd775fced9fe",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e994afa6-4b0e-4343-816f-aaa3558c85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPRL_TF(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes_m1,num_codes_m2):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "        self.m1_embedding = nn.Embedding(num_codes_m1+1,embedding_dim=128)\n",
    "        encoder_layer1 = nn.TransformerEncoderLayer(d_model=128, nhead=2)\n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=2)\n",
    "        \n",
    "        self.m2_embedding_num = nn.Linear(in_features=104, out_features=96)\n",
    "        self.m2_embedding_cat = nn.Embedding(num_codes_m2+1,embedding_dim=32)\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(d_model=128, nhead=2)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x1_pos, masks1_pos, x2_pos, masks2_pos, x1_neg=None, masks1_neg=None, x2_neg=None, masks2_neg=None):\n",
    "        batch_size = x1_pos.shape[0]\n",
    "        \n",
    "        # FIRST MODAL pos\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x1_pos = self.m1_embedding(x1_pos)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x1_pos = sum_embeddings_with_mask(x1_pos, masks1_pos)\n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output1_pos = self.transformer_encoder1(x1_pos)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n1_pos = get_last_visit(output1_pos, masks1_pos)\n",
    "        \n",
    "        # SECOND MODAL pos\n",
    "        num_x2_pos = x2_pos[:,:,:-3]\n",
    "        cat_x2_pos = x2_pos[:,:,-3:]\n",
    "        num_x2_pos = self.m2_embedding_num(num_x2_pos.float())\n",
    "        cat_x2_pos = self.m2_embedding_cat(cat_x2_pos)\n",
    "        cat_x2_pos = cat_x2_pos.sum(dim=2)\n",
    "        x2_pos = torch.concat([num_x2_pos, cat_x2_pos], dim=-1)\n",
    "        output2_pos = self.transformer_encoder2(x2_pos)\n",
    "        true_h_n2_pos = get_last_visit(output2_pos, masks2_pos)\n",
    "        \n",
    "        # FOR PREDICTION\n",
    "        # concat hidden stats and pass through \n",
    "        true_h_n_pos = torch.maximum(true_h_n1_pos, true_h_n2_pos)\n",
    "        x = self.relu(self.fc1(true_h_n_pos))\n",
    "        out_pos = self.fc2(x).view(batch_size)\n",
    "        \n",
    "        if x1_neg != None:\n",
    "            # FIRST MODAL neg\n",
    "            # 1. Pass the sequence through the embedding layer;\n",
    "            x1_neg = self.m1_embedding(x1_neg)\n",
    "            # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "            x1_neg = sum_embeddings_with_mask(x1_neg, masks1_neg)\n",
    "            # 3. Pass the embegginds through the RNN layer;\n",
    "            output1_neg = self.transformer_encoder1(x1_neg)\n",
    "            # 4. Obtain the hidden state at the last visit.\n",
    "            true_h_n1_neg = get_last_visit(output1_neg, masks1_neg)\n",
    "\n",
    "            # SECOND MODAL pos\n",
    "            num_x2_neg = x2_neg[:,:,:-3]\n",
    "            cat_x2_neg = x2_neg[:,:,-3:]\n",
    "            num_x2_neg = self.m2_embedding_num(num_x2_neg.float())\n",
    "            cat_x2_neg = self.m2_embedding_cat(cat_x2_neg)\n",
    "            cat_x2_neg = cat_x2_neg.sum(dim=2)\n",
    "            x2_neg = torch.concat([num_x2_neg, cat_x2_neg], dim=-1)\n",
    "            output2_neg = self.transformer_encoder2(x2_neg)\n",
    "            true_h_n2_neg = get_last_visit(output2_neg, masks2_neg)\n",
    "\n",
    "            # FOR TRAINING/Calculating loss\n",
    "            # lm1m2\n",
    "            exp_pos = torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_pos, true_h_n2_pos) / TAU)\n",
    "            lm1m2_denom = 32 * torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_pos, true_h_n2_neg) / TAU)\n",
    "            lm1m2 = -torch.log(exp_pos/lm1m2_denom)\n",
    "            # lm2m1\n",
    "            lm2m1_denom = 32 * torch.exp(torch.nn.functional.cosine_similarity(true_h_n1_neg, true_h_n2_pos) / TAU)\n",
    "            lm2m1 = -torch.log(exp_pos/lm2m1_denom)    \n",
    "            return out_pos, lm1m2, lm2m1\n",
    "            \n",
    "            \n",
    "        \n",
    "        # prediction mode\n",
    "        return out_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3643780-b2a1-4ed7-98d1-b0a709fab198",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, los_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23165054-d585-4524-ab2c-f1a933edb5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "061aa975-348a-46e6-a8ea-9d9fd528e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f3a921d-8f24-48c0-8baa-4de7d371577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "            x1_neg, masks1_neg, x2_neg, masks2_neg, y = next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cebb812b-f909-4a68-97c6-b23b0bc27dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdprltf = SDPRL_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd01b9c6-c140-443c-9d50-92318f27e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(sdprltf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6834eb-df92-441b-9143-028444604375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(sdprltf, train_loader, val_loader, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdf57f-fec4-4264-93d4-3d74c65407dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(sdprlrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d445b-78df-4fb0-97f7-3964d0321414",
   "metadata": {},
   "source": [
    "MORT ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b29f7d7-4c82-47f1-abca-4fa462ec03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataset = CustomMMDataset(demo_iseqs, demo_vseqs, mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31e931eb-7e3b-484d-afc4-277817c85bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27577\n",
      "Length of val dataset: 6895\n"
     ]
    }
   ],
   "source": [
    "split = int(len(mm_dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(mm_dataset) - split]\n",
    "train_dataset, val_dataset = random_split(mm_dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c909c12-ca68-4fa6-b147-2bb4c8ab5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "afd64be0-9e1a-4f03-a941-9575d1d49ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "x1_pos, masks1_pos, x2_pos, masks2_pos,\\\n",
    "            x1_neg, masks1_neg, x2_neg, masks2_neg, y = next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "397467e1-3956-4383-b749-e21439ec07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdprltf = SDPRL_TF(14+49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17a886cb-5c0b-4684-b981-7654eb2f6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(sdprltf.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739340a-1a39-493e-b001-63fefb71d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "train(sdprltf, train_loader, val_loader, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9713e71-abc0-4ff3-a80b-488cdfdbba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7944453064391002\n",
      "0.21529256966372448\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc,auprc = eval_model(sdprlrnn, val_loader)\n",
    "print(roc_auc)\n",
    "print(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ab8f0-239f-4da9-8ad3-b143046e2ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd0208-db09-4366-b621-39fb645ea02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df148dda-af38-404f-becd-d00f1555cd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
